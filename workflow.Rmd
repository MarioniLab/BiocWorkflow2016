---
title: A worfklow for low-level analyses of single-cell RNA-seq data
author: 
    - name: Aaron T. L. Lun
      affiliation: Cancer Research UK Cambridge Institute, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom
    - name: John C. Marioni
      affiliation: Cancer Research UK Cambridge Institute, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom; EMBL European Bioinformatics Institute, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SD, United Kingdom
date: 19 February 2016
vignette: >
    %\VignetteIndexEntry{A worfklow for low-level analyses of single-cell RNA-seq data}
    %\VignetteEngine{knitr::rmarkdown}
output: 
    BiocStyle::html_document:
        fig_caption: yes
bibliography: ref.bib
---

```{r style, echo=FALSE, results='hide', message=FALSE}
library(BiocStyle)
library(knitr)
opts_chunk$set(error=FALSE)
opts_chunk$set(fig.width=7, fig.height=7)
opts_chunk$set(dpi=300, dev="png", dev.args=list(pointsize=15))
```

```{r, message=FALSE, echo=FALSE, results='hide'}
library(scran)
library(DESeq2)
library(edgeR)
```

# Introduction

Single-cell RNA sequencing (scRNA-seq) is widely used to measure the genome-wide expression profile of individual cells.
From each cell, mRNA is isolated and reverse transcribed to cDNA for high-throughput sequencing [@stegle2015computational].
This can be done using microfluidics platforms like the Fluidigm C1 [@pollen2014lowcoverage], or with protocols based on microtiter plates like Smart-seq2 [@picelli2014fulllength].
The number of reads mapped to each gene can then be used to quantify its expression in each cell.
Alternatively, unique molecular identifiers (UMIs) can be used to directly measure the number of transcript molecules for each gene [@islam2014quantitative].
Count data can be analyzed via dimensionality reduction and clustering to identify new cell subpopulations, and to detect highly variable genes (HVGs) or differentially expressed genes (DEGs) between subpopulations or conditions.
This provides biological insights at a single-cell resolution that cannot be attained with conventional bulk RNA sequencing of cell populations.

Strategies for scRNA-seq data analysis differ markedly from those for bulk RNA-seq.
One technical reason is that scRNA-seq data is much noisier than bulk data [@brennecke2013accounting;@marinov2014singlecell].
Reliable capture (i.e., conversion) of transcripts into cDNA for sequencing is difficult with the low quantity of RNA in a single cell.
This increases the high frequency of drop-out events where none of the transcripts for a gene are captured.
More PCR amplification cycles are often used to compensate for low input quantities, but this can lead to amplification biases and inflated measures of transcript levels.
Dedicated analysis steps are required to deal with this noise, especially during quality control.
In addition, scRNA-seq data can be used to study cell-to-cell heterogeneity, e.g., to identify new cell subtypes, to characterize differentiation processes, to separate cells based on cell cycle phase, or to identify HVGs driving variability across the population [@vallejos2015basics;@fan2016characterizing;@trapnell2014dynamics].
This is simply not possible with bulk data, such that custom methods are required to perform these analyses. 

This article describes a computational workflow for basic analysis of scRNA-seq data using software packages from the open-source Bioconductor project [@huber2015orchestrating].
Starting from a count matrix, this workflow contains the steps required for quality control to remove problematic cells; normalization of cell-specific biases, with and without spike-ins; cell-cycle phase classification from gene expression data; data exploration to identify putative subpopulations; and finally, HVG and DEG identification to prioritize interesting genes.
The application of different steps in the workflow will be demonstrated on several public scRNA-seq data sets -- one from a study of cell types in the mouse brain, and another from a study of mouse embryonic stem cells (mESCs) cultured under different conditions [@kold2015singlecell].
The aim is to provide a variety of modular usage examples that can be applied to construct custom analysis pipelines.

# A simple analysis on haematopoietic stem cells

## Overview

To introduce most of the concepts of scRNA-seq data analysis, we use a relatively simple data set from a study of haematopoietic stem cells (HSCs) [@wilson2015combined].
Mouse HSCs were purified and 96 cells were sequenced using the Smart-seq2 protocol.
A constant amount of spike-in RNA from the External RNA Controls Consortium (ERCC) was also added to each cell prior to library preparation.
The expression of each gene was quantified by counting the total number of reads mapped to the exonic regions of that gene.
Similarly, the quantity of each spike-in transcript was measured by counting reads mapped to the spike-in reference sequence.
Counts for all genes/transcripts in each cell were obtained from the NCBI Gene Expression Omnibus as a supplementary file under the accession number GSE61533.

For simplicity, we forgo a description of the read processing steps required to generate the count matrix, i.e., read alignment and counting into features.
These steps have been described in some detail elsewhere [@love2015rnaseq], and are largely the same for bulk and single-cell data.
The only additional consideration is that the spike-in information must be included in the pipeline.
Typically, spike-in sequences can be included as additional FASTA files during genome index building prior to alignment, while genomic intervals for both spike-in transcripts and endogenous genes can be concatenated into a single GTF file prior to counting.
For users favouring a R-based approach to read alignment and counting, we suggest using the methods in the `r Biocpkg("Rsubread")` package [@liao2013subread;@liao2014featurecounts].

## Count loading and quality control

The first task is to load the count matrix into memory.
This requires some work to decompress and retreive the data from the Excel format.
Each row of the matrix represents an endogenous gene or a spike-in transcript, and each column represents a single HSC.

```{r}
library(R.utils)
gunzip("GSE61533_HTSEQ_count_results.xls.gz", remove=FALSE)
library(gdata)
all.counts <- read.xls('GSE61533_HTSEQ_count_results.xls', sheet=1, header=TRUE, row.names=1)
dim(incoming)
```

Rows corresponding to spike-in transcripts are identified by the `ERCC` prefix in the row names.
For convenience, the counts for spike-in transcripts and endogenous genes are stored separately in a `SummarizedExperiment` object.
This is done using the `countsToSE` function in the `r Rpackage("scran")`package.

```{r}
library(scran)
spike.in <- grepl('^ERCC', rownames(all.counts))
y <- countsToSE(all.counts[!spike.in,], all.counts[spike.in,])
sum(spike.in)
```

Low-quality cells are defined as those where the endogenous RNA has not been efficiently captured (i.e., converted into cDNA and amplified) during library preparation.
These can be identified as those cells with low library sizes, i.e., a low sum of counts for the endogenous genes.
In general, any cell with a library size that is an order of magnitude lower than the median across all cells can be considered to be of low quality and removed.

```{r}
keep <- y$lib.size >= median(totals)/10
y <- y[,keep]
sum(keep)
```

Another measure of low quality involves the proportion of reads mapped to genes in the mitochondrial genome.
High proportions are indicative of poor-quality cells [@islam2014quantitative], possibly because of increased apoptosis.
Here, the mitochondrial genes have been conveniently marked with a `mt-` prefix, so the proportion can be easily computed for each cell.

```{r mitoplot, fig.cap="Histogram of the proportion of reads mapped to mitochondrial genes across all cells in the HSC data set."}
is.mito <- grepl("^mt-", rownames(y))
mito.props <- colSums(assay(y)[is.mito,])/y$lib.size 
hist(mito.props, xlab="Mitochondrial proportion", ylab="Number of cells", 
    breaks=20, cex.lab=1.4, cex.axis=1.2, main="")
```

The appropriate threshold on the mitochondrial proportion will vary from case to case, depending on the cell type and the experimental protocols.
If we assume that most cells in the data set are not apoptotic, then the threshold can be set to remove obvious outliers from the distribution of proportions.
A threshold value of 0.09 is probably suitable for this data set, based on the previous histogram.

```{r}
keep <- mito.props < 0.09
y <- y[,keep]
sum(keep)
```

Very low-abundance genes are also removed prior to further analysis.
These genes tend to provide little information as the counts are too low for reliable inferences.
In addition, the discreteness of the counts may interfere with downstream statistical procedures, e.g., by compromising the accuracy of asymptotic approximations.
Here, low-abundance genes are defined as those with an average count across cells below 1.
Removing them avoids problems with discreteness and also reduces the amount of computational work.

```{r}
keep <- rowMeans(assay(y)) >= 1
y <- y[keep,] 
sum(keep)
```

## Normalization of cell-specific biases

Read counts are subject to differences in capture efficiency and sequencing depth between cells [@stegle2015computational].
Normalization is required to eliminate these cell-specific biases prior to downstream quantitative analyses.
This is often done by assuming that most genes are not differentially expressed (DE) between cells.
Any systematic difference in count size across the non-DE majority of genes between two cells is assumed to represent bias and is removed by scaling.
More specifically, "size factors" are calculated that represent the extent to which counts should be scaled in each library.

Size factors can be computed with several different approaches, e.g., using the `estimateSizeFactorsFromMatrix` function in the `r Biocpkg("DESeq2")` package [@anders2010differential;@love2014moderated], or with the `calcNormFactors` function [@robinson2010scaling] in the `r Biocpkg("edgeR")` package (this requires an additional multiplication by the library size).
However, single-cell data can be problematic for these bulk data-based methods due to the dominance of low and zero counts.
To overcome this, we pool counts from many cells to increase the count size for accurate size factor estimation [@lun2016much].
Pool-based size factors are then "deconvolved" into cell-based factors for cell-specific normalization.

```{r, warning=FALSE}
y$size.factor <- normalizeBySums(y, sizes=c(20, 40, 60, 80))
summary(y$size.factor)
```

Normalized log-expression values can be computed for use in downstream analyses.
Each value is defined as the log-ratio of each count to the size factor for the corresponding cell (after adding a small prior count to avoid undefined values at zero counts).
Division by the size factor ensures that any cell-specific biases are removed.
The log-transformation is also useful as larger counts can be modelled reasonably well with a log-normal distribution [@law2014voom].
The computed values are stored as an `"exprs"` matrix in the `SummarizedExperiment` object, in addition to the existing count matrix.

```{r}
y <- normalize(y)
assayNames(y)
```

## Data exploration with dimensionality reduction techniques

Dimensionality reduction is often useful to examine major features of the data before more quantitative analyses.
Of particular interest is whether the HSCs partition into distinct subpopulations.
This can be visualized by constructing a principal components analysis (PCA) plot from the normalized log-expression values.
Cells with more similar expression profiles should be located close together on the plot.
All genes are scaled to have unit variances to ensure that low-abundance genes with high variances do not dominate the plot.

```{r pcaplot, fig.cap="PCA plot constructed from normalized log-expression values, where each point represents a cell in the HSC data set."}
out <- prcomp(t(assay(y, "exprs")), scale.=TRUE)
plot(out$x[,1], out$x[,2], xlab="PC1", ylab="PC2", pch=16, 
    main="Expression values", cex.axis=1.2, cex.lab=1.4, main="")
```

Alternatively, the PCA plot can be constructed from a matrix of correlations between cells. 
Two cells with similar expression profiles will exhibit similar patterns of correlations to other cells.
As a result, those two cells will be closer together in the plot.
This approach tends to be more robust to noise and outliers that would dominate a standard PCA plot, at the cost of being less sensitive to differences in the expression profiles.
Scaling of the variances is not performed here as the correlation patterns are directly comparable between cells.

```{r pcacorplot, fig.cap="PCA plot constructed from a matrix of Spearman rank correlations between cells, where each point represents a cell in the HSC data set."}
cor.mat <- cor(assay(y, "exprs"), method="spearman")
out <- prcomp(cor.mat, scale.=TRUE)
plot(out$x[,1], out$x[,2], xlab="PC1", ylab="PC2", pch=16, main="Correlations")
```

Another popular approach to dimensionality reduction is the _t_-stochastic neighbour embedding (_t_-SNE) method [@van2008visualizing].
A fast implementation is provided by the `Rtsne` function in the `r CRANpkg("Rtsne")` package, and can be applied to the normalized log-expression values for all cells.
Note that, unlike PCA, _t_-SNE is a stochastic method -- users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible.
It is also advisable to test different settings of the "perplexity" parameter as this will affect the distribution of points in the low-dimensional space.

```{r tsneplot, fig.cap="_t_-SNE plot constructed from normalized log-expression values, where each point represents a cell in the HSC data set.", fig.width=12, fig.height=6}
library(Rtsne)
set.seed(100)
par(mfrow=c(1,3), cex.axis=1.2, cex.lab=1.3, cex.main=1.4)
transposed <- t(assay(y, "exprs"))
out <- Rtsne(transposed, perplexity=5)
plot(out$Y[,1], out$Y[,2], xlab="t-SNE1", ylab="t-SNE2", main="Perplexity=5", pch=16)
out <- Rtsne(transposed, perplexity=10)
plot(out$Y[,1], out$Y[,2], xlab="t-SNE1", ylab="t-SNE2", main="Perplexity=10", pch=16)
out <- Rtsne(transposed, perplexity=20)
plot(out$Y[,1], out$Y[,2], xlab="t-SNE1", ylab="t-SNE2", main="Perplexity=20", pch=16)
```

In general, _t_-SNE tends to work better than PCA for large data sets with complex substructure.
However, this comes at the cost of more computational effort and complexity.
For this particular data set, both methods suggest that there is no strong partionining into distinct subpopulations.
Of course, there are many dimensionality reduction techniques that we have not considered here but could also be used, e.g., multidimensional scaling.
Greater resolution of differences can also be achieved by applying these methods on expression values for a selected set of HVGs.
This focuses on genes that are driving heterogeneity and potential substructure. 

## Classification of cell cycle phase 

We use the prediction method described by @scialdone2015computational to classify cells into cell cycle phases based on the gene expression data.
Using a training data set, the sign of the difference in expression between two genes was computed for each pair of genes.
Pairs were chosen with changes in the sign across cell cycle phases.
Cells in a test data set can then be classified into the appropriate phase, based on whether the observed sign is consistent with one phase or another.
This is achieved using the `cyclone` function on a pre-trained set of gene pairs for mouse data.
(Some additional work is necessary to match the gene symbols in the data to the Ensembl annotation in the set of pairs.)

```{r phaseplot, message=FALSE, fig.cap="Cell cycle phase scores from applying the pair-based classifier on the HSC data set, where each point represents a cell. "}
mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran"))
library(org.Mm.eg.db)
anno <- select(org.Mm.eg.db, keys=rownames(y), keytype="SYMBOL", column="ENSEMBL")
ensembl <- anno$ENSEMBL[match(rownames(y), anno$SYMBOL)]
keep <- !is.na(ensembl)
assignments <- cyclone(assay(y, "exprs")[keep,], mm.pairs, gene.names=ensembl[keep])
plot(assignments$score$G1, assignments$score$G2M, xlab="G1 score", ylab="G2/M score",
    pch=16, cex.axis=1.2, cex.lab=1.4, main="")
```

Cells are classified as being in G1 phase if the G1 score is above 0.5; in G2/M phase if the G2/M score is above 0.5; and in S phase, if neither is above 0.5.
Here, the vast majority of cells are classified as being in G1 phase.
We will focus on these cells for the downstream analysis.
Cells in other phases are removed to avoid potential confounding effects from cell cycle-induced differences.
Alternatively, if a non-negligible number of cells are in other phases, we can use the assigned phase as a blocking factor in downstream analyses.
This protects against cell cycle effects without discarding a substantial amount of information.

```{r}
g1.only <- assignments$score$G1 > 0.5
y <- y[,g1.only]
```

Pre-trained classifiers are available in `r Rpackage("scran")` for human and mouse experiments.
These tend to work reasonably well -- the classifier is non-parametric with few assumptions, and the transcriptional program associated with cell cycling should be mostly conserved across different cell types and biological settings.
However, users can construct a custom classifier from their own training data using the `sandbag` function.
This may be necessary for other model organisms where pre-trained classifiers are not available.

## Identifying HVGs from the normalized log-expression 

Identification of HVGs allows us to focus on the genes that are driving heterogeneity across cells.
We first decompose the variance in expression for each gene into biological and technical components.
The latter is estimated by fitting a mean-variance trend to the spike-in transcripts with the `trendVar` function.
Recall that the same set of spike-ins is added in the same quantity to each cell, and thus should exhibit no biological variability.
The biological component of the variance can then be estimated by subtracting the fitted value of the trend from the total variance of each gene in the `decomposeVar` function.

```{r hvgplot, fig.cap="Variance of normalized log-expression values for each gene in the HSC data set, plotted against the average log-expression. The red line represents the mean-dependent trend in the technical variance of the spike-in transcripts (also highlighted as red points)."}
var.fit <- trendVar(y, trend="loess", span=0.2)
var.out <- decomposeVar(y, var.fit)
plot(var.out$mean, var.out$total, pch=16, cex=0.6, xlab="Variance", ylab="Mean")
points(var.fit$mean, var.fit$var, col="red", pch=16)
o <- order(var.out$mean)
lines(var.out$mean[o], var.out$tech[o], col="red", lwd=2)
```

The top HVGs are identified by ranking genes on their biological components.
This can be used to prioritize interesting genes for further investigation.

```{r}
top.hvgs <- order(var.out$bio, decreasing=TRUE)
write.table(file="hsc_hvg.tsv", var.out[top.hvgs,], sep="\t", quote=FALSE, col.names=NA)
head(var.out[top.hvgs,])
```

Of course, there are many ways of defining HVGs, e.g., by using the coefficient of variation [@kolod2015singlecell;@kim2015characterizing], through the dispersion parameter in the negative binomial distribution [@mccarthy2012differential], or as a proportion of total variability [@vallejos2015basics].
We use the variance of the log-expression values because the log-transformation provides some protection against genes with strong expression in only one or two outlier cells.
This ensures that the set of top HVGs is not dominated by genes with (mostly uninteresting) outlier expression patterns.

## Identifying correlated gene pairs with Spearman's rho

Another useful procedure is to identify the HVGs that are highly correlated with one another.
This distinguishes between HVGs caused by random noise and those involved in driving systematic differences between subpopulations.
Gene pairs with significantly large positive or negative values for Spearman's rho are identified using the `correlatePairs` function.
Note that we only apply this function for the top set of HVGs -- doing so for all possible gene pairs would require too much computational time and may prioritize uninteresting genes that have strong correlations but low variance, e.g., tightly co-regulated house-keeping genes.

```{r}
set.seed(100)
var.cor <- correlatePairs(y[top.hvgs[1:200],])
write.table(file="hsc_cor.tsv", var.cor, sep="\t", quote=FALSE, col.names=NA)
head(var.cor)
```

The top set of correlated HVGs can be prioritized for follow-up studies.
For example, a set of correlated genes can be used as markers to validate the existence of a subpopulation of cells with a specific expression pattern, e.g., via fluorescence-activated cell sorting, immunohistology or RNA flourescence _in situ_ hybridization.
Negatively correlated pairs may be particularly useful in this respect, as they provide a clear "negative control" signal for cells outside of the subpopulation.

Larger sets of correlated genes can be assembled by treating genes as nodes in a graph and each pair of genes with significantly large correlations as an edge.
In this manner, an undirected graph can be constructed using methods in the `r Biocpkg("RBGL")` package.
Highly connected subgraphs can then be identified and defined as gene sets.
This provides a convenient summary of the gene-gene correlation structure in this data set.

```{r}
require(RBGL)
sig.cor <- var.cor$FDR <= 0.05
g <- ftM2graphNEL(cbind(var.cor$gene1, var.cor$gene2)[sig.cor,], W=NULL, V=NULL, edgemode="undirected")
cl <- highlyConnSG(g)$clusters
cl <- cl[order(lengths(cl), decreasing=TRUE)]
cl <- cl[lengths(cl) > 2]
cl
```

## Using correlated HVGs for further data exploration

```{r}
# Looking at the top genes.
heat.vals <- assay(y, "exprs")
heat.vals <- heat.vals - rowMeans(heat.vals)
is.sig <- var.cor$FDR <= 0.05 
chosen <- unique(c(var.cor$gene1[is.sig], var.cor$gene2[is.sig]))
require(gplots)
pdf("out.pdf", height=12, width=6)
heatmap.2(heat.vals[chosen,], col=bluered, symbreak=TRUE, trace='none', cexRow=0.5)
dev.off()
```




```{r, eval=FALSE}
incoming <- read.table('GSE60361_C1-3005-Expression.txt.gz', header=TRUE)
keep <- !duplicated(incoming[,1])
all.counts <- incoming[keep,-1]
rownames(all.counts) <- incoming[keep,1]

totals <- colSums(all.counts)
okay.libs <- totals > 1e4 
all.counts <- all.counts[,okay.libs]

library(scran)
y <- countsToSE(all.counts) 
clusters <- quickCluster(y)
y$size.factor <- normalizeBySums(y, cluster=clusters)
y <- normalize(y)

# Cell cycle phase classification.
mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran"))
library(org.Mm.eg.db)
anno <- select(org.Mm.eg.db, keys=rownames(y), keytype="SYMBOL", column="ENSEMBL")
ensembl <- anno$ENSEMBL[match(rownames(y), anno$SYMBOL)]
keep <- !is.na(ensembl)
assignments <- cyclone(assay(y, "exprs")[keep,], mm.pairs, gene.names=ensembl[keep], BPPARAM=MulticoreParam(5))
plot(assignments$score$G1, assignments$score$G2M)

var.fit <- trendVar(y, trend="loess", use.spikes=FALSE)
var.out <- decomposeVar(y, var.fit)

set.seed(100)
top.hvgs <- order(var.out$bio, decreasing=TRUE)[1:200]
var.cor <- correlatePairs(y[top.hvgs,])

heat.vals <- assay(y, "exprs")
heat.vals <- heat.vals - rowMeans(heat.vals)
is.sig <- var.cor$FDR <= 0.05 
chosen <- unique(c(var.cor$gene1[is.sig], var.cor$gene2[is.sig]))
heatmap.2(heat.vals[chosen,], col=bluered, symbreak=TRUE, trace='none', cexRow=0.5)
```

# Choosing between normalization with and without spike-ins

## Overview
Scaling normalization strategies for scRNA-seq data can be broadly divided into two classes.
The first class assumes that there exists a subset of genes that are not differentially expressed between samples.
This subset can be manually specified to contain house-keeping genes, or it can be empirically identified under the assumption that most genes are not DE [@anders2010differential;@robinson2010scaling].
Any systematic difference in the counts across the non-DE subset is treated as technical bias and is eliminated by scaling.
The second class of normalization strategies uses spike-in RNA of known composition and abundance.
Specifically, the same quantity of spike-in RNA is added to each cell, captured into libraries and sequenced along with endogenous transcripts [@stegle2015computational].
Differences in the coverage of the spike-in transcripts can only be due to cell-specific biases, e.g., in capture efficiency or sequencing depth.
Scaling normalization is then applied to equalize spike-in coverage across cells.

The choice between these two normalization strategies depends on the biology of the cells and the features of interest.
If there is no reliable house-keeping set, and if the majority of genes are expected to be DE, then spike-in normalization may be the only option for removing technical biases.
Spike-in normalization should also be used if differences in the total RNA content of individual cells are of interest.
This is because the same amount of spike-in RNA is added to each cell, such that the relative quantity of endogenous RNA can be easily quantified in each cell.
For non-DE normalization, any change in total RNA content will affect all genes in the non-DE subset, such that it will be treated as bias and removed.
(This may be desirable if changes in total content are _not_ interesting.)
Similarly, if spike-ins are not present or if they cannot be added to each cell in a reliable manner, then non-DE normalization should be applied.

## Computing size factors for spike-in normalization

The use of spike-in normalization can be demonstrated on a simple data set comparing mESCs and mouse embryonic fibroblasts (MEFs) [@islam2011characterization].
A constant quantity of synthetic spike-in RNA was added to each cell, and spike-in transcripts were sequenced and counted along with transcripts from endogenous genes.
Quantification of gene expression was performed in this manner for 48 mESCs, 44 MEFs and 4 negative controls.
We obtain a table of read counts from NCBI GEO using the accession GSE29087, and load them into a `SummarizedExperiment` object for further manipulation.

```{r}
counts <- read.table("GSE29087_L139_expression_tab.txt.gz", skip=6, 
    sep='\t', row.names=1)[,-c(1:6)]
is.spike <- grepl("SPIKE", rownames(counts))
sum(is.spike)
library(scran)
y <- countsToSE(counts[!is.spike,], counts[is.spike,])
y$grouping <- factor(rep(c("mESC", "MEF", "Neg"), c(48, 44, 4)))
dim(y)
```

We remove low-quality libraries that have low total counts for the endogenous genes or high proportions of reads mapped to genes on the mitochondrial genome.
This removes a number of libraries, including those for the negative controls.
We also remove lowly expressed genes with an average count below 1.

```{r}
totals <- colSums(assay(y))
is.mito <- grepl("^mt-", rownames(y))
okay.libs <- totals >= 1e5 & colSums(assay(y)[is.mito,])/totals < 0.1 
y <- y[,okay.libs]
sum(okay.libs)
keep.gene <- rowMeans(assay(y)) >= 1
y <- y[keep.gene,] 
sum(keep.gene)
```

We compute size factors using the `normalizeBySpikes` function.
The size factor for each cell is proportional to the total number of reads mapped to spike-in transcripts.
The aim is to normalize the counts such that the total spike-in coverage is the same across cells.
These values are stored in the `SummarizedExperiment` object as previously described, and downstream analyses can be applied.

```{r}
sf.spike <- normalizeBySpikes(y)
y$size.factor <- sf.spike
````

Note that mESCs have consistently larger size factors compared to MEFs.
This is due to the fact that the former contain substantially less endogenous RNA than the latter [@islam2011characterization].
Larger size factors result in smaller normalized expression values for mESCs compared to MEFs, reflecting the decrease in total RNA content in the former.
These differences are lost when a normalization method based on a non-DE majority is applied, such as that in the `r Biocpkg("DESeq2")` package [@anders2010differential].

```{r}
y$grouping <- droplevels(y$grouping)
boxplot(split(sf.spike, y$grouping), log="y", cex.axis=1.2, cex.lab=1.4)
library(DESeq2)
y2 <- y
y2$size.factor <- estimateSizeFactorsForMatrix(assay(y2))
boxplot(split(y2$size.factor, y$grouping), log="y", cex.axis=1.2, 
    cex.lab=1.4, ylim=range(sf.spike))
```

## Effect of spike-in normalization on downstream analyses

Preservation of differences in total RNA content has some notable implications for downstream analyses.
For DEG detection between cell types, the change in total RNA content will be incorporated into the log-fold change.
This means that more DEGs are generally detected upon spike-in normalization compared to normalization based on a non-DE majority.
In addition, there tends to be a greater imbalance in the number of up- and downregulated genes, as the change in total RNA content occurs in one direction for all genes.
This is demonstrated below by comparing the DE results between the two normalization strategies. 

```{r}
library(edgeR)
d <- DGEList(assay(y, "counts"))
d$offset <- log(y$size.factor)
design <- model.matrix(~y$grouping)
d <- estimateDisp(d, design)
fit <- glmFit(d, design)
res <- glmLRT(fit)
summary(decideTestsDGE(res))
d2 <- d
d2$offset <- log(y2$size.factor)
d2 <- estimateDisp(d2, design)
fit2 <- glmFit(d2, design)
res2 <- glmLRT(fit2)
summary(decideTestsDGE(res2))
```

One might question the relevance of DE that is driven by changes in total RNA content between cell types.
Clearly, however, there will be a change in transcript abundance when the total amount of RNA in each cell changes.
Thus, from a technical perspective, the detection of DE for those genes is appropriate.
Biologically, the relevance of DE would depend on whether an increase in transcript molecules results in more molecular activity, e.g., due to increased production of protein product.
While this relation seems trivially true, it is easy to imagine situations where it is not the case, e.g., when the translation machinery is saturated such that a global increase in transcript molecules has no effect.

<!-- Also, if there's variability in cell sizes within groups, this'll reduce power to detect DEGs. -->

For HVG detection, any variability in total RNA content is incorporated into the variance of each gene.
This means that it will increase the size of the biological component of the variance.
However, the magnitude of the increase will be the same for each gene as the variance of RNA content is constant.
As such, even though the biological component will increase, the ranking of HVGs will be largely preserved. 

```{r}
y <- normalize(y)
trend <- trendVar(y, trend="loess", span=0.8)
components <- decomposeVar(y, trend)
y2 <- normalize(y2)
trend2 <- trendVar(y2, trend="loess", span=0.8)
components2 <- decomposeVar(y2, trend2)
```

Most dimensionality reduction and clustering procedures tend to be robust to the choice of normalization method.
This is because cells with different RNA content also tend to have different transcript compositions.
Thus, they would be (correctly) separated based on their gene expression profiles, regardless of whether changes in total RNA content were preserved after normalization.
Indeed, correlation-based methods are completely insensitive to normalization as the value of the correlation is not dependent on the scaling of expression values within each cell.

```{r, fig.width=12, fig.height=6}
par(mfrow=c(1,2))
col <- c("red", "blue")[y$grouping]
out <- prcomp(t(assay(y, "exprs")), scale.=TRUE)
plot(out$x[,1], out$x[,2], col=col, pch=16, xlab="PC1", ylab="PC2")
out2 <- prcomp(t(assay(y2, "exprs")), scale.=TRUE)
plot(out2$x[,1], out2$x[,2], col=col, pch=16, xlab="PC1", ylab="PC2")
```

## Classifying cells into cell cycle phases

## Detecting DEGs between pre-defined groups of cells

# Conclusions

# Software availability

```{r}
sessionInfo()
```

# Author contributions

A.T.L.L. developed the workflow on all data sets.

# Competing interests

No competing interests were disclosed.

# Grant information

CRUK core funding (SW73) to J.C.M.

# Acknowledgements

Aaron is pretty awesome.

# References

