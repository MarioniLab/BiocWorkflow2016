# Quality control comments

## Improving resolution with log-transformed QC metrics

By improving resolution, I refer to compression of high values and expansion of the range of low values.
The former reduces the MAD relative to the median, such that "3 MADs away" is a sensible statistic.
The latter makes it easier to distinguish between outliers and the edge of the distribution of acceptable values.
Or, from another perspective, the large MAD that is driven by greater variability at high values isn't relevant to the threshold choice at low values when working on the raw scale.
By transforming to the log scale, the variance is stabilised across the real line.

On a more conceptual note, the MAD is necessary to account for genuine biological heterogeneity in these metrics.
That's why we don't use a hard-and-fast fold-change threshold from the median, as this would be too aggressive or not aggressive enough in some situations.

## Interpreting the proportion mapped to spike-ins

It shouldn't matter too much if it's the proportion against total counts, or proportion against endogenous counts.
This is because we're not measuring an increase in mitochondrial/spike-in counts, but rather, a depletion of endogenous RNA.
If endogenous RNA decreases in low-quality cells, the mitochondrial/spike-in proportions against the total count should both increase.
We don't have to worry about effects of e.g. an increase in mitochondrial counts affecting the proportion of spike-in counts.

The absolute value of the spike-in proportion can also be used for QC.
You would want about 5-10% of the reads going to the spike-ins.
If this is not the case, it suggests that you need to alter the dilution.
You can also compare the observed proportions to the expected values, which can be calculated if RNA quantification was done on the cells beforehand.
Neither of these approaches provide a threshold for filtering, but they do tell you if the experiment went well or not.

Also, we don't use the logit transform for the proportions, even though on the raw scale we could theoretically end up with a above-unity threshold.
This is because the logit transform compresses changes within the middle of the [0,1] range.
This reduces the resolution for where the threshold would usually be.

## Assumptions of outlier identification

There's an implicit assumption that these technical metrics are homogeneous across cells.
That won't be true for extreme cases like erythrocytes, where the "outliers" would just be a genuine biological clustering.
It also won't be true for batches with known differences in experimental processing, e.g., sequencing at different depth or had different amounts of spike-in added.

# Cell cycle classification comments

## Relationship with filtering

It seems to more sense to apply `cyclone` before filtering out low-abundance genes.
This is because phase-specific genes that are not expressed in *any* cell will still be useful for classification.

## Explaining poor performance on the brain data set

Another possible contributor to poor performance is the difference in the cells used for training and those in the test data.
If certain expression patterns are associated with the cell cycle in the training set (of mouse embryonic stem cells), these may be incorporated into the classifier.
However, if those patterns are not associated with the cell cycle in the test data, their inclusion will add noise without providing any phase information.
This will lead to a deterioration in the accuracy of the classifier.
In general, this is unlikely to be a major issue as the cell cycle should be a conserved process across many lineages and conditions.

Furthermore, there may be misclassification due to a large number of cells being in G0.
In theory, these should be closest to G1 but they may be different enough that you'd get a low G1 score, making them show up as S-like or even G2M.

# Filtering comments

## Justification in the context of HVG detection

It could be argued that you don't need to do filtering on abundance if you're going to select on HVGs anyway.
This is because the HVG screen would throw out low-abundance genes, so you might as well skip the abundance filter.
The disadvantage is if the HVG screen depends on significance, in which case the low abundances would increase the severity of the MTC.
This could be mild (~20% increase in the p-value) or quite severe (2-3-fold), depending on how many annotated genes are retained.
The worst case occurs if you had mild contamination so that every gene had a count of one - you'd retain too many uninteresting genes if you just filtered on non-zero totals.

To this end, the choice of filter is guided by the upper bound of the variance compared to the expected amount of technical noise.
The variance is bounded as the counts must be non-negative, so a gene with a low mean count can only achieve a particular variance (of the logs).
Below, we use linear programming to identify the maximum variance of logs at any specified mean of logs/mean of counts. 
We optimize over the proportions of cells with counts from 0 to 200.

```{r}
library(lpSolve)
counts <- 0:200
lmeanfun <- log2(counts+1)
meanfun <- counts

sumfun <- rep(1, length(counts))
sumval <- 1
posfun <- diag(length(counts))

collected <- list()
for (meanval in 1:100/100) {
    for (lmeanval in 1:100/100) {
        lvarfun <- (log2(counts+1) - lmeanval)^2
        out <- lp(direction="max", objective.in=lvarfun, 
                const.mat=rbind(meanfun, lmeanfun, sumfun, posfun), 
                const.dir=c("=", "=", "=", rep(">=", length(counts))),
                const.rhs=c(meanval, lmeanval, sumval, rep(0, length(counts))))
        if (out$objval==0) { next }
        collected[[length(collected)+1]] <- c(lmeanval, meanval, out$objval)
    }
}
collected <- do.call(rbind, collected)
```

In the HVG data set, an average count of 1 corresponds to a maximum variance of around 1.
The technical noise is around about this much anyway, so it makes no sense to keep genes with lower average counts, because they just wouldn't be called as HVGs.
We could be more stringent and require some minimum increase above the technical noise, but we won't do that.

```{r}
plot(collected[,1], collected[,3], xlim=c(0, 1), ylim=c(0, 1))
comp <- read.table("hsc_hvg.tsv", header=TRUE)
o <- order(comp$mean)
lines(comp$mean[o], comp$tech[o], col="red", pch=16, cex=0.5)
```

Similar logic applies in the brain data set.
Here, the technical noise is lower, so we set a smaller filter.
It depends somewhat on whether you want to extrapolate the trend with `rule=2` or as a line passing through the origin.
In both cases, though, it's safe to say that the maximum variances are comparable to the trend.

```{r}
plot(collected[,1], collected[,3],xlim=c(0, 1), ylim=c(0, 1), cex=ifelse(collected[,2] < 0.2, 1, 0))
comp <- read.table("brain_hvg.tsv", header=TRUE)
o <- order(comp$mean)
lines(comp$mean[o], comp$tech[o], col="red", pch=16, cex=0.5)
```

Note that directly filtering on the log-means is _technically_ correct, in that it's independent of HVG detection.
However, it's possible for genes to have very small log-means and very large variances, e.g., if you have a rare subpopulation with a very strongly expressed gene.
Filtering on the log-means would discard such genes, which we don't want to do.

## Justification on biological grounds

Genes with low counts may be due to transcriptional leakage or mapping/sequencing errors (especially pseudogenes).
This makes them uninteresting as they're unlikely to be related to any genuine biology.
One could even go so far as to say that genes that aren't expressed much are likely to be irrelevant to the phenotype.
Of course, there's likely to be a couple of low-abundance genes that (a) are important and (b) vary noticeably across cells.
However, this is probably the exception rather than the rule; getting hammered by the MTC and other statistical problems to squeeze these genes out seems suboptimal.

## Saving the original data

It's also good practice to save the full data set before filtering.
This is because the filtered genes in one context (e.g., for the full data set) might not be of the greatest interest in another context (e.g., with a subset of the data).
A prime example would be in iterative clustering, or when looking at subgroups.
In such cases, it's useful to filter and process everything afresh.

# Normalization comments

## Why use log-transformed normalized counts?

The log-transformation provides some measure of variance stabilization for NB-distributed counts with a constant dispersion but a variable mean. 
This seems better than square-rooting it, which only works for the Poisson.

We don't bother imputing the drop-outs, because we don't know whether a zero is a true dropout or a genuine zero.
Besides, it seems somewhat circular if you try to impute from the same data, rather than using some external data (e.g., in variant calling).
The simpler approach seems to be to try to properly model it downstream.

## Additional normalization for confounding effects

The percentage of variance explained by an uninteresting technical effect has obvious effects on HVG detection.
However, it also affects the correlation between genes because it represents some common underlying factor.
For a factor with increasing percentage explained 'p', Pearson's correlation will increase to 1, e.g., by 'p' at a true correlation of zero.
(This is based on adding Normal variates to each other, with one part representing the true expression of each gene and the other representing the common factor.
These two components are independent of each other within each gene -- you can then decompose the covariance between genes to get the correlation-proportion relationship.)

If 'p' is decently large (>10%), we're likely to have problems, so the corresponding factor will be need to be regressed out.
Of course, blocking on factors introduces more assumptions and points of failure to the analysis (e.g., linearity in the covariates).
If the model is misspecified, you can end up with spurious patterns in the residuals - possibly larger than that caused by the technical bias in the first place.
This is compounded by the presence of zeroes, non-normality and heteroskedasticity, etc.
Thus, blocking should be performed sparingly, rather than being performed by default.

## Misspecifying the model when running `removeBatchEffect`

Comparing residuals between blocking factor levels can run into problems if population structure varies between levels.
Imagine a case where we have two batches, NO batch effect and different proportions of the same cell types in each batch.
Computing residuals can result in spurious differences within each cell type for genes that are DE between cell types.
This is because the batch-specific average will be different due to the different composition of each batch.
It's actually worse than a completely confounding effect, as at least total confounding would just result in loss of differences and a false negative.

This seems to only affect situations where residuals need to compared across levels.
For variance calculations, residual effects are evaluated in terms of their total size, so this is less of an issue.
In fact, the whole point is to pick up highly variable genes that aren't captured well by the model, so this is a good thing.
For correlations with one-way layouts, comparisons are done within each level so it should be fine.
There are problems for additive designs, but we knew that already.

With all that being said, correction is probably the lesser of two evils if you have a strong batch effect that compromises the visualization.
It shouldn't matter for technical effects that are largely orthogonal to population structure (e.g., balanced designs).
Problems would only occur when you try to regress out uninteresting biological effects that might have some composition differences.

# HVG detection comments

## Trend fitting to variances of the log-counts

The mean-variance trend for log-expression values is more complex and difficult to fit than that of other approaches.
But we can do it, so it's a technical challenge rather than a philosophical one.

The thin line to the left is a mathematical artifact when you have discrete counts.
- the variance will increase as a concave quadratic when you can only choose between 0 and 1 (low 'p' increases with fixed 'n' in a binomial RV).
- the variance will increase as a convex quadratic with the mean, if variance is driven by an outlier and everything else is 0.

I'm not sure how much effort it's worth to extract HVG information from this part of the plot, as low counts are dominated by Poisson sampling noise.

## Motivating the threshold for significance

Specifically, for a standard normal, the square root of the expected squared distance between two cells would be sqrt(2). 
So, if you set the standard error to 1/sqrt(2), the distance would become 1 (i.e., 2-fold change).
Setting this threshold avoids selecting genes with high fold changes above the technical variance, but small absolute total variances.
Such genes are more likely to be true positives but also less likely to be strongly variable and biologically interesting.

(Of course, genes driving separation of rare subpopulations would have variances below 0.5.
Even if the log-fold changes between subpopulations is strong, the average squared difference across all cells would be low.
However, you've got to draw the line somewhere, otherwise you'll end up with lots of genes with really homogenous expression profiles, and irrelevant noise during PCA, etc.
I guess this is just the price that needs to be paid before you can go on and do clustering to explicitly identify subpopulations.)

The threshold is a bit informal, but that's okay.
The DM, for example, is no better, and Brennecke has that +0.25 value that isn't very interpretable).
In and of themselves, HVGs are not of interest -- rather, they prioritise genes for more interesting analyses, e.g., clustering, correlation and gene set analyses.
If HVG detection is considered as a screen, it is better to focus on potentially interesting (but possibly false) genes rather than true and uninteresting ones.

To this end, it is permissible to relax the FDR for detecting HVGs, especially if you didn't get anything interesting things with a low threshold.
Of course, this would also increase the amount of genes dominated by technical noise later on, so it's not preferable if you can avoid it.
One could argue that this is not problematic if you just get more low-variability genes, as these don't contribute much to relative differences betwen cells.
However, technical noise is still high (in absolute terms) and there are a lot more of them, so it would probably still mess up the results.

In any case, the p-values calculated here are probably more appropriate than those from Brennecke.
Log-expression values are a lot more normal-looking than the raw counts, due to the skew of the latter.

## Pros and con of using log-count variances over CV^2^

Log-count variances are also more consistent with downstream applications.
For example, PCA and t-SNE are applied on the log-values, as is visualization of expression with boxplots or violin plots.
Making the latter with genes identified as HVGs from CV2 would give outliers that get shrunk upon log-transformation.
Indeed, the variance of the log-values provides a measure of the log-fold change between cells, which is arguably more relevant than the absolute differences in expression.

With CV^2^ you pick up a lot of genes expressed in few cells, which can be a pain for downstream analyses.
Most directly, it means that the HVG list isn't easily interpretable if you have to manually weed out a lot of uninteresting genes.
It also disrupts identification of trajectories:

```{r}
set.seed(100)

nsamples <- 1000  
standardSetup <- function(ngenes, nspikes) {
    total.genes <- ngenes + nspikes
    means <- 2^runif(total.genes, -2, 10)
    dispersions <- 10/means + 0.2
    counts <- matrix(rnbinom(total.genes*nsamples, mu=means, size=1/dispersions), ncol=nsamples)
    is.spike <- logical(total.genes)
    is.spike[seq_len(nspikes)] <- TRUE
    return(list(counts=counts, is.spike=is.spike))
}
    
# All of the standard genes...
sim <- standardSetup(1000, 100)
counts <- sim$counts
is.spike <- sim$is.spike

# Adding some substructure - in this case, a trajectory.
# The CV2 is resistant to scaling, so don't worry too much about the absolute values.
affect <- 20
counts[sum(is.spike)+1:affect,] <- matrix(1:nsamples, ncol=nsamples, nrow=affect, byrow=TRUE)

# Adding some outliers.
subcounts <- matrix(0, 200, nsamples)
subcounts[sample(length(subcounts), 200)] <- 500 
counts <- rbind(counts, subcounts)
is.spike <- c(is.spike, logical(nrow(subcounts)))

# CV2 picks up none of the HVGs - the CV2 is too small.
# Might be due to a continuum, where the variance just can't get particularly large
# (compared to bimodal expression patterns).
out <- technicalCV2(counts, is.spike, sf.cell=rep(1, nsamples), sf.spike=rep(1, nsamples))
lcounts <- log2(counts+1)
is.sig <- which(out$FDR <= 0.05 & !is.na(out$FDR))
sum(is.sig >= 101 & is.sig <= 120)
sig.out <- lcounts[is.sig,]

# Log-variances picks up all of the HVGs but not the outliers.
fit <- trendVar(lcounts[is.spike,])
ref <- decomposeVar(lcounts, fit)
is.sig <- which(ref$FDR <= 0.05 & !is.na(ref$FDR) & ref$bio > 0.5)
sum(is.sig >= 101 & is.sig <= 120)
sig.ref <- lcounts[is.sig,]

par(mfrow=c(1,2))
pr.out <- prcomp(t(sig.out))
pr.ref <- prcomp(t(sig.ref))
plot(pr.out$x[,1])# pr.out$x[,2])
plot(pr.ref$x[,1])#, pr.ref$x[,2])

library(destiny)
dm.out <- DiffusionMap(t(sig.out))
dm.ref <- DiffusionMap(t(sig.ref))
plot(dm.out$DC1)# pr.out$x[,2])
plot(dm.ref$DC1)#, pr.ref$x[,2])
```

... and clustering of large but subtle subpopulations (i.e., subpopulations consisting of many cells, which are poorly separated from neighbouring subpopulations). 
This is because the relevant genes don't get picked up if the mean is decently large -- or, if they do get picked up, they get dominated by noise from outliers.
Of course, this is all relative -- for strong trajectories or clear subpopulations, both methods should perform well.

```{r}
set.seed(100)

# More of the standard genes...
sim <- standardSetup(1000, 100)
counts <- sim$counts
is.spike <- sim$is.spike

# Adding some substructure - in this case, some clusters.
extrasim <- standardSetup(50, 0)
subcounts <- extrasim$counts
subcounts[,1:500] <- subcounts[,1:500] * 2
counts <- rbind(counts, subcounts)
is.spike <- c(is.spike, logical(nrow(subcounts)))

# Adding some outliers.
subcounts <- matrix(0, 200, nsamples)
subcounts[sample(length(subcounts), 200)] <- 500 
counts <- rbind(counts, subcounts)
is.spike <- c(is.spike, logical(nrow(subcounts)))

# CV2 picks up mostly outliers, not the HVGs.
out <- technicalCV2(counts, is.spike, sf.cell=rep(1, nsamples), sf.spike=rep(1, nsamples))
lcounts <- log2(counts+1)
is.sig <- which(out$FDR <= 0.05 & !is.na(out$FDR))
sum(is.sig >= 1101 & is.sig <= 1150)
sig.out <- lcounts[is.sig,]
clust.out <- kmeans(t(sig.out), 2)

# Log-variances picks up the HVGs but not the outliers.
fit <- trendVar(lcounts[is.spike,])
ref <- decomposeVar(lcounts, fit)
is.sig <- which(ref$FDR <= 0.05 & !is.na(ref$FDR) & ref$bio > 0.5)
sum(is.sig >= 1101 & is.sig <= 1150)
sig.ref <- lcounts[is.sig,]
clust.ref <- kmeans(t(sig.ref), 2)

lapply(split(clust.out$cluster, rep(c("A", "B"), each=500)), table)
lapply(split(clust.ref$cluster, rep(c("A", "B"), each=500)), table)
```

On the other hand, as a result of the robustness to outliers, detection power of the log-based method is reduced for HVGs driven by rare subpopulations.
CV^2^-based methods do better, as shown below where HVGs corresponding to rare subpopulations are detected with lower mean expression in those cells.
This performance gap increases for genes that are expressed at some level and upregulated in the rare subpopulation.
It is difficult to detect these with the log-based method, given that uniquely expressed genes with a near-infinite fold-change are already hard to pick up. 

```{r}
set.seed(100)
    
# All of the standard genes...
sim <- standardSetup(5000, 500)
counts <- sim$counts
is.spike <- sim$is.spike

# Adding some genes only present in a small (1%) subpopulation.
subcounts <- matrix(0, 500, nsamples)
my.means <- 1:500*10
chosen <- nrow(counts) + seq_along(my.means)
subcounts[,1:10] <- my.means
counts <- rbind(counts, subcounts)
is.spike <- c(is.spike, logical(length(my.means)))

# CV2 can detect it at a count of 10-20.
out <- technicalCV2(counts, is.spike, sf.cell=rep(1, nsamples), sf.spike=rep(1, nsamples))
plot(out$mean, out$cv2, log="xy")
points(out$mean, out$trend, col="red", pch=16, cex=0.5)
my.means[min(which(out$FDR[chosen] <= 0.05))]

# Log-variances detects it at a count of 100 (around 1000 if you also require biological variances above 0.5). 
lcounts <- log2(counts+1)
fit <- trendVar(lcounts[is.spike,])
ref <- decomposeVar(lcounts, fit)
plot(ref$mean, ref$total)
o <- order(ref$mean)
lines(ref$mean[o], ref$tech[o], col="red", lwd=2)
my.means[min(which(ref$FDR[chosen] <= 0.05))]
my.means[min(which(ref$FDR[chosen] <= 0.05 & ref$bio[chosen]>0.5))]
```

On the flip side, CV2-based methods are less effective at detecting HVGs for subpopulations characterised by a loss of expression.
This is probably because the mean is already large, which limits the scale of the change in the CV^2^.
The log-based method can detect such genes, provided that expression is high (around 500) in all other cells.
To some extent, this mitigates the loss of power for expressed genes in rare subpopulations, allowing the cells to be correctly classified.

```{r}
set.seed(100)

# All of the standard genes, again.
sim <- standardSetup(5000, 500)
counts <- sim$counts
is.spike <- sim$is.spike

# Adding some genes not present in a small (1%) subpopulation.
extrasim <- standardSetup(500, 0)
subcounts <- extrasim$counts
subcounts[,1:10] <- 0
chosen <- nrow(counts) + seq_len(nrow(subcounts))
counts <- rbind(counts, subcounts)
is.spike <- c(is.spike, logical(nrow(subcounts)))

# CV2 detects a few.
out <- technicalCV2(counts, is.spike, sf.cell=rep(1, nsamples), sf.spike=rep(1, nsamples))
plot(out$mean, out$cv2, log="xy")
points(out$mean, out$trend, col="red", pch=16, cex=0.5)
sum(out$FDR[chosen] <= 0.05)

# Log-variances detects more.
lcounts <- log2(counts+1)
fit <- trendVar(lcounts[is.spike,])
ref <- decomposeVar(lcounts, fit)
plot(ref$mean, ref$total)
o <- order(ref$mean)
lines(ref$mean[o], ref$tech[o], col="red", lwd=2)
sum(ref$FDR[chosen] <= 0.05)
sum(ref$FDR[chosen] <= 0.05 & ref$bio[chosen]>0.5)
```

In short, the final recommendation is to use the log-based methods for initial exploration, because it's better at recovering major features in the data.
You can then switch to `technicalCV2` when pulling out rare subpopulations.

## Biological interpretion of HVGs

We can interpret HVGs as genes where each cell has an (unknown) true expression that varies across cells, e.g., due to subpopulations or across a continuum.
This can also be extended across time, e.g., due to transcriptional bursting or circadian rhythms.
In other words, HVGs are equivalent to DE genes for unknown subsets of cells.
Validating whether the variability is functionally relevant becomes straightforward, as we can just KO or overexpress the gene.
This is equivalent to the strategy that would be used to validate the underlying DE, if the subsets were known.
One can also see this as seeing what happens after reducing the variance by coercing everyone to be lowly or highly-expressing.
While it won't preserve the population mean, this is largely irrelevant if HVGs are to equivalent to DEGs anyway.
(Such a task -- reducing variability while preserving the mean -- would be monumentally difficult.)

## Reasoning behind iterative HVG and clustering

The set of HVGs detected within a cluster may be more relevant.
This is because you can detect HVGs at greater power if you didn't have uninvolved, constantly-expressing cells dragging down the variance/correlations.
Similarly, you'd get rid of genes that are HVGs between clusters but are not within the cluster, which wouldn't help with internal clustering.

# Correlation comments

## Using HVGs as a pre-screen

Obviously, if you don't pre-screen, you'll get a whole lot of genes that are driven by technical noise.
This should be random and reduce the correlations (and power) -- or, if not random, then definitely uninteresting.

An alternative analytical approach would be a method that detects correlations and HVGs at the same time, where strong correlations would offset low variances for gene detection.
The problem is that, taken to its logical conclusion, this would probably pick up a large web of genes that have strong correlations with low total variances.
This is probably uninteresting, e.g., residual technical effects (like cell size) or uninteresting biology (ribosome-related correlations).

## How to use the correlation results

The idea is to use the correlated gene pairs without having to rely on clustering.
This is closer to the raw data and avoids the errors and ambiguities introduced by clustering.
Negative correlations are particularly powerful as they provide definitive signals for both opposing clusters (or both ends of a trajectory).
This gets around problems in validation where double positive/negative signals might just be due to differences in cell accessibility, permeability, etc.

Of course, relying solely on correlations is also a bit less interpretable, as the identities of the cells in the subpopulations are not explicitly set.
It is also limited to the top set of HVGs, whereas DE between clusters can be checked between all genes.
(Although the rest of the genes are unlikely to have strong DE, otherwise they would have been HVGs.)
Nonetheless, using correlated genes should enrich for structure and reduce the amount of noise going into clustering and dimensionality reduction.

At the very least, one can use the correlation results to back up the (less reliable but more interpretable) higher-level analyses.
So if you get a result from the latter that you mightn't trust (due to uncertainty of clustering, etc.), you can fall back to the correlations if it shows up there.

## Setting an absolute value on the correlation

We could also set a threshold on the absolute value of the correlation.
This is useful when you have lots of cells, which gives you (too much) power to detect non-zero correlations.
It is also valid without further work -- unlike log-fold changes in DE, the absolute correlation here directly determines the p-value.
Thus, you can threshold on the correlation without affecting FDR control, because loss of elements with higher p-values just means the FDR is lower across the rest.
However, I'm disinclined to recommend this explicitly, as you would lose power to detect subtle correlations driving minor subpopulations.
This would defeat the purpose of having lots of cells to improve power to detect those subpopulations.

## Using all HVGs in the brain data set

I also switched to using all HVGs, rather than the top 500 as published.
This is because if you have lots of heterogeneity, genes corresponding to relatively weaker effects are not visible if they get excluded from the top 500.
This occurs even if those effects are actually absolutely large, leading to the inability to detect obvious substructure.
In any case, it actually doesn't take that long, so we might as well just do it using all genes.

# Clustering comments

## Choice of clustering method

Ward's method seems to work well, but complete linkage would also probably do a good job here.
The problem with method selection is that the "best" method depends on the unknown nature of the underlying data.
Ward and complete linkage assume compact clusters, but this might not be the case, e.g., density-based methods would do better for irregular shapes.
This might suggest that ensemble or consensus clustering would perform best.

The issue is with the interpretability of whatever clusters crawl out at the end.
If an assigment only occurs with a minority of methods, should it be discarded, even if those methods focus on particularly pertinent aspects of the data?
This is likely to occur if you use substantially different clustering methods, given that the use of similar methods would defeat the purpose.
Upon summarization, these minority assignments would be discarded and power would be lost relative to application of the minority methods by themselves.

Rather, the main utility of a consensus method is that it tells you which clusters are the most robust with respect to variability from the choice of method.
These clusters can be considered conservative as you need everyone to detect them, resulting in some loss of power.
However, if you assume that each method is affected by noise in different ways, then the consensus clusters are effectively denoised, which is nice.

## Why not bootstrapping?

Standard bootstrapping requires IID genes in order to generate bootstrap replicates of the original data.
This is not the case, which makes it difficult to interpret the bootstrap probabilities on an absolute scale.
Doing it correctly would require block resampling to account for correlations -- hence the difficulty.

## Using diffusion maps

Pseudotime coordinates can be extracted for DE analyses with edgeR/DESeq, a la empirical clustering.
This might be more robust than clustering for continuous trajectories where the cluster boundaries would be more or less arbitrary.
Of course, this depends much on the quality of the trajectory reconstruction.

## Visual artifacts in the heatmap

Note that stripes, rather than blocks, are likely to be visual artifacts.
This is because smaller cells have less stable expression and accumulate red/blue colours, while larger cells get more white.
Obviously, if there are problems with normalization, it'll show up here as well.
In general, these can be ignored; if they're not artifacts, then they'll be impossible to validate.

## Identifying marker genes per cluster

DE genes between subpopulations are not necessarily marker genes.
The latter requires that the gene be consistently expressed (or not) in all cells of one subpopulation compared to the other.
This means that the variance needs to be modelled across cells.
For DE genes, any change will do, even if it only appears in a small percentage of cells of the subpopulation.
This requires modelling of the variance across replicate instances of the entire subpopulation.

That being said, testing for differential expression is not the best approach to identify marker genes.
This is because marker genes are defined not just by a change in mean expression, but also low variability.
Just testing for the former doesn't guarantee that it's not variable -- if the difference is strong enough, the null hypothesis will be rejected.
The reasoning above only favours the rejection of genes with low variability across the population, which may or may not improve the ranking of candidate markers.

Simultaneously testing for both criterions might seem like the solution.
However, this raises the question of how much variability is too much, and the desirable trade-off with strong DE.
The resulting rankings may favour genes with weaker DE but less variability, especially for high-abundance genes expressed in both groups that aren't really markers.

```r
set.seed(100)
library(statmod)
ngroup <- 50
design <- model.matrix(~rep(LETTERS[1:2], each=ngroup))
design0 <- cbind(rep(1, nrow(design)))

# Scenario 1: strong difference, highly variable.
# This is arguably a strong candidate for a marker gene.
y1 <- c(integer(ngroup), rnbinom(ngroup, mu=100, size=1)) 
sum(y1==0) - ngroup # only one cell in the second group is zero.
LR1 <- glmnb.fit(design0, y1, dispersion=1, offset=0)$deviance - 
       glmnb.fit(design, y1, dispersion=1, offset=0)$deviance

# Scenario 2: weaker difference, lowly variable.
# This is not a particularly good marker gene.
y2 <- c(rnbinom(ngroup, mu=50, size=100), 
        rnbinom(ngroup, mu=100, size=100)) 
LR2 <- glmnb.fit(design0, y2, dispersion=0.01, offset=0)$deviance - 
       glmnb.fit(design, y2, dispersion=0.01, offset=0)$deviance
 
# Despite that, the second gene has a greater LR than the first.
LR1
LR2
```

In short, whether you sum across cells or model cell-to-cell variability directly, you'll still get bad candidate markers in the rankings.
Strong markers should appear at the top no matter what you do, so perhaps that's reassuring.

# Cell cycle phase correction comments

An alternative approach uses genes that have annotated functions in cell cycling and division.
We extract all genes associated with the relevant GO terms and use them to construct a PCA plot for the brain dataset.
Figure ((braincyclepca)) contains three clusters that may correspond to distinct phases of the cell cycle.
This can be determined explicitly by identifying marker genes for each cluster as previously described, and checking whether each marker has known phase-specific expression with resources such as [Cyclebase](http://www.cyclebase.org) [@santos2015cyclebase].

```{r, echo=FALSE, results='hide', message=FALSE, eval=FALSE}
library(org.Mm.eg.db)
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))
```

```{r braincyclepca, fig.width=10, fig.height=5, fig.cap="PCA plot of the brain dataset, using only genes with annotated functions in cell cycling or division.", eval=FALSE}
ccgenes <- select(org.Mm.eg.db, keys=c("GO:0022403", "GO:0051301"), keytype="GOALL", column="SYMBOL")
sce <- readRDS("brain_data.rds")
chosen.genes <- which(rownames(sce) %in% ccgenes$SYMBOL)
plotPCA(sce, feature_set=chosen.genes) + fontsize 
```

We can also identify hidden factors of variation across the annotated genes using `r Biocpkg("RUVSeq")`.
This assumes that, if all cells were in the same phase of the cell cycle, there should be no DE across cells for genes associated with the cell cycle.
Any systematic differences between cells are incorporated into the `W` matrix containing the factors of unwanted variation.
These factors can then be included as covariates in the design matrix to absorb cell cycle effects in the rest of the dataset.
We set `k=2` here to capture the variation corresponding to the two principal components in Figure ((braincyclepca)).

```{r, eval=FALSE}
library(RUVSeq)
ruv.out <- RUVg(exprs(sce), isLog=TRUE, cIdx=chosen.genes, k=2)
head(ruv.out$W)
```

In general, we prefer using the `cyclone`-based approach for phase identification and blocking.
This is because the expression of cell cycle genes may be affected by other biological/experimental factors at the single-cell level.
As a result, the inferred factors of variation may include interesting differences between cells, such that blocking on those factors would result in loss of detection power.
`cyclone` calls the phase for each cell separately and is more robust to systematic (non-cell-cycle-related) differences between cells.

The obvious example would be if cells in the same phase had different amounts of expression for cell cycle genes, corresponding to some other factor (e.g., treatment).
This would get picked up as a hidden factor of variation, leading to loss of power to detect that other factor.
In contrast, cyclone wouldn't care as the relative amounts of expression within each cell would be the same for all cells, leading to correct calling of phase.


